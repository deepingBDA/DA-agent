import os
import asyncio
import json
import uuid
import platform
import nest_asyncio
import shutil
from typing import Dict, List, Optional, Any
from pydantic import BaseModel
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from contextlib import asynccontextmanager
from dotenv import load_dotenv
import logging

from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, AIMessageChunk, ToolMessage
from langchain_mcp_adapters.client import MultiServerMCPClient
from utils import astream_graph, random_uuid
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig

# Windows í˜¸í™˜ì„± ì„¤ì •
if platform.system() == "Windows":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# ì¤‘ì²© ì´ë²¤íŠ¸ ë£¨í”„ í—ˆìš©
nest_asyncio.apply()

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)

# ì„¤ì • íŒŒì¼ ê²½ë¡œ
CONFIG_FILE_PATH = "config.json"
UPLOAD_DIR = "uploads"  # ì—…ë¡œë“œ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬

# ì ˆëŒ€ ê²½ë¡œ ê³„ì‚°
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ABSOLUTE_UPLOAD_DIR = os.path.join(BASE_DIR, UPLOAD_DIR)

# ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
if not os.path.exists(ABSOLUTE_UPLOAD_DIR):
    os.makedirs(ABSOLUTE_UPLOAD_DIR)

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """<ROLE>
You are a smart agent with an ability to use tools. 
You are an agent that strengthens offline stores.
You will be given a question and you will use the tools to answer the question.
Pick the most relevant tool to answer the question. 
If you are failed to answer the question, try different tools to get context.
If the user does not specify a period (start date and end date), automatically use the previous week (Mondayâ€“Sunday relative to today) as the default period for all queries.
Your answer should be very polite and professional.
You must provide actionable insights to the user.
You should use tools to obtain specific numbers, then make suggestions based on those numbers.
</ROLE>

----

<INSTRUCTIONS>
Step 1: Analyze the question
- Analyze user's question and final goal.
- If the user's question is consist of multiple sub-questions, split them into smaller sub-questions.

Step 2: Pick the most relevant tool
- Pick the most relevant tool to answer the question.
- If you are failed to answer the question, try different tools to get context.

Step 3: Answer the question
- Answer the question in the same language as the question.
- Your answer should be very polite and professional.

Step 4: Provide the source of the answer(if applicable)
- If you've used the tool, provide the source of the answer.
- Valid sources are either a website(URL) or a document(PDF, etc).

Step 5: Present the data visibly
- Format the query result with clear line breaks, logical indentation, and any effective visual markers (dashes, bullets, arrows, emojis, etc.) to maximize readability

Guidelines:
- If you've used the tool, your answer should be based on the tool's output(tool's output is more important than your own knowledge).
- If you've used the tool, and the source is valid URL, provide the source(URL) of the answer.
- Skip providing the source if the source is not URL.
- Answer in the same language as the question.
- Answer should be concise and to the point.
- Avoid response your output with any other information than the answer and the source.
- Avoid generic or general advice. Always provide specific, data-driven recommendations.
- Include concrete numbers, percentages, or metrics to support your suggestions.
- Provide actionable steps that can be immediately implemented rather than vague guidance.
</INSTRUCTIONS>

----

<OUTPUT_FORMAT>
(concise answer to the question)

**Source**(if applicable)
- (source1: valid URL)
- (source2: valid URL)
- ...
</OUTPUT_FORMAT>
"""

# ëª¨ë¸ í† í° ì •ë³´
OUTPUT_TOKEN_INFO = {
    "gpt-4o": {"max_tokens": 16384, "temperature": 0.1},
    "o3": {"max_tokens": 16384, "temperature": None},  # o3ëŠ” temperature ì§€ì›í•˜ì§€ ì•ŠìŒ
}

# ì„¤ì • ë¡œë“œ í•¨ìˆ˜
def load_config_from_json():
    """
    ì„¤ì •ì„ JSON íŒŒì¼ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.
    íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Returns:
        dict: ë¡œë“œëœ ì„¤ì •
    """
    default_config = {
        "get_current_time": {
            "command": "python",
            "args": ["./mcp_server_time.py"],
            "transport": "stdio"
        }
    }
    
    try:
        if os.path.exists(CONFIG_FILE_PATH):
            with open(CONFIG_FILE_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        else:
            # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ìƒì„±
            save_config_to_json(default_config)
            return default_config
    except Exception as e:
        print(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return default_config

# ì„¤ì • ì €ì¥ í•¨ìˆ˜
def save_config_to_json(config):
    """
    ì„¤ì •ì„ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        config (dict): ì €ì¥í•  ì„¤ì •
    
    Returns:
        bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
    """
    try:
        with open(CONFIG_FILE_PATH, "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        return True
    except Exception as e:
        print(f"ì„¤ì • íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        return False

# MCP í´ë¼ì´ì–¸íŠ¸ì™€ ì—ì´ì „íŠ¸ ì €ì¥ì†Œ
mcp_clients = {}
agents = {}
conversation_histories = {}
agent_models = {}  # ìŠ¤ë ˆë“œë³„ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì €ì¥
tool_count = 0  # ì „ì—­ ë³€ìˆ˜ë¡œ tool_count ì„ ì–¸

# ìš”ì²­ ë° ì‘ë‹µ ëª¨ë¸ ì •ì˜
class Message(BaseModel):
    role: str
    content: str
    attachment_path: Optional[str] = None  # ì²¨ë¶€ íŒŒì¼ ê²½ë¡œ ì¶”ê°€

class MessageResponse(BaseModel):
    messages: List[Message]
    thread_id: str

class QueryRequest(BaseModel):
    query: str
    thread_id: Optional[str] = None
    model: str = "gpt-4o"
    timeout_seconds: int = 120
    recursion_limit: int = 100

class QueryResponse(BaseModel):
    response: str
    tool_info: Optional[str] = None
    thread_id: str

class ToolRequest(BaseModel):
    tool_config: Dict[str, Any]

class SettingsResponse(BaseModel):
    tool_count: int
    available_models: List[str]
    current_config: Dict[str, Any]

class ThreadResponse(BaseModel):
    thread_id: str

class FileUploadResponse(BaseModel):
    file_path: str
    file_name: str
    file_size: int

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì„¤ì • ë¡œë“œ
    if not os.path.exists(ABSOLUTE_UPLOAD_DIR):
        os.makedirs(ABSOLUTE_UPLOAD_DIR)
    print(f"ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ê²½ë¡œ: {ABSOLUTE_UPLOAD_DIR}")
    yield
    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ëª¨ë“  MCP í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ
    for client in mcp_clients.values():
        try:
            await client.__aexit__(None, None, None)
        except Exception as e:
            print(f"MCP í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ ì˜¤ë¥˜: {str(e)}")

# íŒŒì¼ ì—…ë¡œë“œ í¬ê¸° ì œí•œì„ 300MBë¡œ ì„¤ì •
app = FastAPI(title="MCP Tool Agent API", lifespan=lifespan)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” íŠ¹ì • ì¶œì²˜ë§Œ í—ˆìš©ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì • (ì—…ë¡œë“œ íŒŒì¼ ì•¡ì„¸ìŠ¤ìš©)
app.mount("/uploads", StaticFiles(directory=ABSOLUTE_UPLOAD_DIR), name="uploads")

# HTML ë³´ê³ ì„œ ì„œë¹™ ì„¤ì •
REPORT_DIR = os.path.join(BASE_DIR, "report")
if not os.path.exists(REPORT_DIR):
    os.makedirs(REPORT_DIR, exist_ok=True)
app.mount("/reports", StaticFiles(directory=REPORT_DIR), name="reports")

# ì—ì´ì „íŠ¸ ì´ˆê¸°í™” í•¨ìˆ˜
async def initialize_agent(thread_id: str, model: str = "gpt-4o", mcp_config=None):
    """
    MCP ì„¸ì…˜ê³¼ ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

    Args:
        thread_id: ëŒ€í™” ìŠ¤ë ˆë“œ ID
        model: ì‚¬ìš©í•  ëª¨ë¸
        mcp_config: MCP ë„êµ¬ ì„¤ì • ì •ë³´(JSON). Noneì¸ ê²½ìš° ê¸°ë³¸ ì„¤ì • ì‚¬ìš©

    Returns:
        bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
    """
    try:
        global tool_count  # ì „ì—­ ë³€ìˆ˜ ì‚¬ìš© ì„ ì–¸
        
        # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ê°€ ìˆìœ¼ë©´ ì‚­ì œ
        if thread_id in mcp_clients:
            del mcp_clients[thread_id]
        
        if mcp_config is None:
            # ì„¤ì • íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ
            mcp_config = load_config_from_json()
        
        # ìƒˆë¡œìš´ API ì‚¬ìš© ë°©ì‹ ì ìš©
        client = MultiServerMCPClient(mcp_config)
        tools = await client.get_tools()  # get_tools ë©”ì„œë“œê°€ ì´ì œ ë¹„ë™ê¸°ì‹ì…ë‹ˆë‹¤
        tool_count = len(tools)  # ì „ì—­ ë³€ìˆ˜ì— í• ë‹¹
        
        print(f"ğŸ” [TOOLS] ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ê°œìˆ˜: {tool_count}")
        for i, tool in enumerate(tools[:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"ğŸ” [TOOLS] ë„êµ¬ {i+1}: {getattr(tool, 'name', 'name ì—†ìŒ')}")
            if hasattr(tool, 'description'):
                print(f"  ì„¤ëª…: {tool.description[:100]}...")
        if len(tools) > 3:
            print(f"ğŸ” [TOOLS] ... ì™¸ {len(tools)-3}ê°œ ë”")
        
        # OpenAI ëª¨ë¸ ì‚¬ìš©
        model_info = OUTPUT_TOKEN_INFO[model]
        llm_kwargs = {
            "model": model,
            "max_tokens": model_info["max_tokens"],
        }
        
        # temperatureê°€ ì„¤ì •ë˜ì–´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€ (o3ëŠ” temperature ì§€ì›í•˜ì§€ ì•ŠìŒ)
        if model_info["temperature"] is not None:
            llm_kwargs["temperature"] = model_info["temperature"]
            
        llm = ChatOpenAI(**llm_kwargs)
        
        agent = create_react_agent(
            llm,
            tools,
            checkpointer=MemorySaver(),
            prompt=SYSTEM_PROMPT,
        )
        
        # í´ë¼ì´ì–¸íŠ¸ì™€ ì—ì´ì „íŠ¸ ì €ì¥
        mcp_clients[thread_id] = client
        agents[thread_id] = agent
        conversation_histories[thread_id] = []
        agent_models[thread_id] = model  # í˜„ì¬ ëª¨ë¸ ì €ì¥
        
        return True
    except Exception as e:
        print(f"ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        return False

# ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í´ë˜ìŠ¤
class StreamingResponse:
    def __init__(self):
        self.accumulated_text = []
        self.accumulated_tool = []
        print(f"accumulated_text: {self.accumulated_text}")
        print(f"accumulated_tool: {self.accumulated_tool}")
    
    def callback(self, message: dict):
        # ë””ë²„ê¹…ì„ ìœ„í•œ ë©”ì‹œì§€ ë¡œê¹…
        # print(f"ë©”ì‹œì§€ íƒ€ì…: {type(message)}")
        # print(f"ë©”ì‹œì§€ í‚¤: {message.keys() if hasattr(message, 'keys') else 'No keys'}")
        
        message_content = message.get("content", None)
        # print(f"ë©”ì‹œì§€ ë‚´ìš© íƒ€ì…: {type(message_content)}")
        
        if isinstance(message_content, AIMessageChunk):
            content = message_content.content
            # ì½˜í…ì¸ ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš° (Claude ëª¨ë¸ ë“±ì—ì„œ ì£¼ë¡œ ë°œìƒ)
            if isinstance(content, list) and len(content) > 0:
                message_chunk = content[0]
                # í…ìŠ¤íŠ¸ íƒ€ì…ì¸ ê²½ìš° ì²˜ë¦¬
                if message_chunk["type"] == "text":
                    self.accumulated_text.append(message_chunk["text"])
                # ë„êµ¬ ì‚¬ìš© íƒ€ì…ì¸ ê²½ìš° ì²˜ë¦¬
                elif message_chunk["type"] == "tool_use":
                    if "partial_json" in message_chunk:
                        tool_content = message_chunk["partial_json"]
                        formatted_tool = f"**ë„êµ¬ ì´ë¦„**: `{tool_content.get('name', 'ì•Œ ìˆ˜ ì—†ìŒ')}`\n\n**ì…ë ¥ê°’**:\n```json\n{json.dumps(tool_content.get('args', {}), indent=2, ensure_ascii=False)}\n```\n"
                        self.accumulated_tool.append(formatted_tool)
                    else:
                        tool_call_chunks = message_content.tool_call_chunks
                        tool_call_chunk = tool_call_chunks[0]
                        formatted_tool = f"**ë„êµ¬ í˜¸ì¶œ ì •ë³´**:\n\n```json\n{json.dumps(tool_call_chunk, indent=2, ensure_ascii=False)}\n```\n"
                        self.accumulated_tool.append(formatted_tool)
            # tool_calls ì†ì„±ì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬ (OpenAI ëª¨ë¸ ë“±ì—ì„œ ì£¼ë¡œ ë°œìƒ)
            elif (
                hasattr(message_content, "tool_calls")
                and message_content.tool_calls
                and len(message_content.tool_calls[0]["name"]) > 0
            ):
                # ğŸ” ì›ì‹œ tool_calls ë°ì´í„° í™•ì¸
                print(f"ğŸ” [RAW] message_content.tool_calls ì „ì²´: {message_content.tool_calls}")
                print(f"ğŸ” [RAW] tool_calls ê°œìˆ˜: {len(message_content.tool_calls)}")
                print(f"ğŸ” [RAW] ì²« ë²ˆì§¸ tool_call ì›ë³¸: {message_content.tool_calls[0]}")
                print(f"ğŸ” [RAW] ì²« ë²ˆì§¸ tool_call íƒ€ì…: {type(message_content.tool_calls[0])}")
                tool_call_info = message_content.tool_calls[0]
                tool_name = tool_call_info.get("name", "ì•Œ ìˆ˜ ì—†ìŒ")
                tool_args = tool_call_info.get("arguments", {})
                
                # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë„êµ¬ ì •ë³´ í¬ë§·íŒ…
                formatted_tool = f"**ë„êµ¬ ì´ë¦„**: `{tool_name}`\n\n**ì…ë ¥ê°’**:\n```json\n{json.dumps(tool_args, indent=2, ensure_ascii=False)}\n```\n"
                self.accumulated_tool.append(formatted_tool)
                
                # ì½˜ì†”ì— ë„êµ¬ í˜¸ì¶œ ì •ë³´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                print(f"ë„êµ¬ í˜¸ì¶œ ê°ì§€: {tool_name}")
                print(f"ì›ë³¸ tool_call_info ì „ì²´: {tool_call_info}")
                print(f"tool_call_info.keys(): {list(tool_call_info.keys())}")
                
                # args í•„ë“œ ì§ì ‘ í™•ì¸
                if 'args' in tool_call_info:
                    print(f"ì§ì ‘ args í•„ë“œ: {tool_call_info['args']}")
                    print(f"args íƒ€ì…: {type(tool_call_info['args'])}")
                
                # arguments í•„ë“œ í™•ì¸
                if 'arguments' in tool_call_info:
                    print(f"arguments í•„ë“œ: {tool_call_info['arguments']}")
                    print(f"arguments íƒ€ì…: {type(tool_call_info['arguments'])}")
                
                print(f"getìœ¼ë¡œ íŒŒì‹±ëœ ì¸ì: {tool_args}")
                print(f"íŒŒì‹±ëœ ì¸ì íƒ€ì…: {type(tool_args)}")
                
                if isinstance(tool_args, str):
                    try:
                        parsed_args = json.loads(tool_args)
                        print(f"JSON íŒŒì‹±ëœ ì¸ì: {parsed_args}")
                    except Exception as e:
                        print(f"JSON íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ ë¬¸ìì—´: {repr(tool_args)}, ì˜¤ë¥˜: {e}")
            # ë‹¨ìˆœ ë¬¸ìì—´ì¸ ê²½ìš° ì²˜ë¦¬
            elif isinstance(content, str):
                self.accumulated_text.append(content)
            # ìœ íš¨í•˜ì§€ ì•Šì€ ë„êµ¬ í˜¸ì¶œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            elif (
                hasattr(message_content, "invalid_tool_calls")
                and message_content.invalid_tool_calls
            ):
                tool_call_info = message_content.invalid_tool_calls[0]
                formatted_tool = f"**ìœ íš¨í•˜ì§€ ì•Šì€ ë„êµ¬ í˜¸ì¶œ**:\n\n```json\n{json.dumps(tool_call_info, indent=2, ensure_ascii=False)}\n```\n"
                self.accumulated_tool.append(formatted_tool)
            # tool_call_chunks ì†ì„±ì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            elif (
                hasattr(message_content, "tool_call_chunks")
                and message_content.tool_call_chunks
            ):
                tool_call_chunk = message_content.tool_call_chunks[0]
                formatted_tool = f"**ë„êµ¬ í˜¸ì¶œ**:\n\n```json\n{json.dumps(tool_call_chunk, indent=2, ensure_ascii=False)}\n```\n"
                self.accumulated_tool.append(formatted_tool)
            # additional_kwargsì— tool_callsê°€ ìˆëŠ” ê²½ìš° ì²˜ë¦¬ (ë‹¤ì–‘í•œ ëª¨ë¸ í˜¸í™˜ì„± ì§€ì›)
            elif (
                hasattr(message_content, "additional_kwargs")
                and "tool_calls" in message_content.additional_kwargs
            ):
                tool_call_info = message_content.additional_kwargs["tool_calls"][0]
                tool_name = tool_call_info.get("name", "ì•Œ ìˆ˜ ì—†ìŒ")
                tool_args = tool_call_info.get("arguments", {})
                
                formatted_tool = f"**ë„êµ¬ ì´ë¦„**: `{tool_name}`\n\n**ì…ë ¥ê°’**:\n```json\n{json.dumps(tool_args, indent=2, ensure_ascii=False)}\n```\n"
                self.accumulated_tool.append(formatted_tool)
                
                # ì½˜ì†”ì— ë„êµ¬ í˜¸ì¶œ ì •ë³´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                print(f"ë„êµ¬ í˜¸ì¶œ ê°ì§€(additional_kwargs): {tool_name}")
                print(f"ì›ë³¸ additional_kwargs ì „ì²´: {message_content.additional_kwargs}")
                print(f"tool_calls ë°°ì—´: {message_content.additional_kwargs.get('tool_calls', [])}")
                
                if message_content.additional_kwargs.get('tool_calls'):
                    first_tool = message_content.additional_kwargs['tool_calls'][0]
                    print(f"ì²« ë²ˆì§¸ tool_call ì „ì²´: {first_tool}")
                    print(f"tool_call keys: {list(first_tool.keys()) if isinstance(first_tool, dict) else 'Not dict'}")
                    
                    # function í•„ë“œ í™•ì¸
                    if 'function' in first_tool:
                        func_info = first_tool['function']
                        print(f"function í•„ë“œ: {func_info}")
                        if 'arguments' in func_info:
                            print(f"function.arguments: {func_info['arguments']}")
                            print(f"function.arguments íƒ€ì…: {type(func_info['arguments'])}")
                
                print(f"getìœ¼ë¡œ íŒŒì‹±ëœ ì¸ì: {tool_args}")
                print(f"íŒŒì‹±ëœ ì¸ì íƒ€ì…: {type(tool_args)}")
                
                if isinstance(tool_args, str):
                    try:
                        parsed_args = json.loads(tool_args)
                        print(f"JSON íŒŒì‹±ëœ ì¸ì: {parsed_args}")
                    except Exception as e:
                        print(f"JSON íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ ë¬¸ìì—´: {repr(tool_args)}, ì˜¤ë¥˜: {e}")
        # ë„êµ¬ ë©”ì‹œì§€ì¸ ê²½ìš° ì²˜ë¦¬ (ë„êµ¬ì˜ ì‘ë‹µ)
        elif hasattr(message_content, 'content') and isinstance(message_content, ToolMessage):
            tool_content = message_content.content
            # ë„êµ¬ ì‘ë‹µ ì •ë³´ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
            formatted_tool = f"**ë„êµ¬ ì‘ë‹µ ê²°ê³¼**:\n\n```\n{tool_content}\n```\n"
            self.accumulated_tool.append(formatted_tool)
            
            # ì½˜ì†”ì— ë„êµ¬ ì‘ë‹µ ì •ë³´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            print(f"ë„êµ¬ ì‘ë‹µ ë‚´ìš©: {tool_content[:200]}..." if len(tool_content) > 200 else tool_content)
        # ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ ì²˜ë¦¬
        elif hasattr(message_content, 'content') and isinstance(message_content.content, str):
            self.accumulated_text.append(message_content.content)
        
        return None
    
    def get_results(self):
        final_text = "".join(self.accumulated_text)
        final_tool = "".join(self.accumulated_tool)
        
        print(f"ìµœì¢… í…ìŠ¤íŠ¸ ê¸¸ì´: {len(final_text)}")
        print(f"ìµœì¢… ë„êµ¬ ì •ë³´ ê¸¸ì´: {len(final_tool)}")
        
        return final_text, final_tool

# ì§ˆë¬¸ ì²˜ë¦¬ í•¨ìˆ˜
async def process_query(thread_id: str, query: str, timeout_seconds=60, recursion_limit=100):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        thread_id: ëŒ€í™” ìŠ¤ë ˆë“œ ID
        query: ì‚¬ìš©ì ì§ˆë¬¸ í…ìŠ¤íŠ¸
        timeout_seconds: ì‘ë‹µ ìƒì„± ì œí•œ ì‹œê°„(ì´ˆ)
        recursion_limit: ì¬ê·€ í˜¸ì¶œ ì œí•œ íšŸìˆ˜

    Returns:
        response: ì—ì´ì „íŠ¸ ì‘ë‹µ ê°ì²´
        final_text: ìµœì¢… í…ìŠ¤íŠ¸ ì‘ë‹µ
        final_tool: ìµœì¢… ë„êµ¬ í˜¸ì¶œ ì •ë³´
    """
    try:
        if thread_id in agents:
            streaming_response = StreamingResponse()
            
            try:                
                messages = [HumanMessage(content=query)]
                print(f"ğŸ” [AGENT] ì—ì´ì „íŠ¸ì—ê²Œ ì „ì†¡í•˜ëŠ” ë©”ì‹œì§€: {query}")
                
                response = await asyncio.wait_for(
                    astream_graph(
                        agents[thread_id],
                        {"messages": messages},
                        callback=streaming_response.callback,
                        config=RunnableConfig(
                            recursion_limit=recursion_limit,
                            thread_id=thread_id,
                        ),
                    ),
                    timeout=timeout_seconds,
                )
            except asyncio.TimeoutError:
                error_msg = f"ìš”ì²­ ì‹œê°„ì´ {timeout_seconds}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                return {"error": error_msg}, error_msg, ""

            print("StreamingResponse ì™„ë£Œ")
            final_text, final_tool = streaming_response.get_results()
            
            return response, final_text, final_tool
        else:
            return {"error": "ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}, "ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", ""
    except Exception as e:
        import traceback
        error_msg = f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n{traceback.format_exc()}"
        return {"error": error_msg}, error_msg, ""

# API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
@app.get("/api/settings")
async def get_settings():
    """í˜„ì¬ ì„¤ì • ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    print("[GET] /api/settings")
    global tool_count  # ì „ì—­ ë³€ìˆ˜ ì‚¬ìš© ì„ ì–¸
    
    config = load_config_from_json()
    
    # MCP í´ë¼ì´ì–¸íŠ¸ê°€ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì„ì‹œ í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ ë„êµ¬ ê°œìˆ˜ í™•ì¸
    if tool_count == 0:
        try:
            # ìƒˆë¡œìš´ API ì‚¬ìš© ë°©ì‹ ì ìš©
            temp_client = MultiServerMCPClient(config)
            tools = await temp_client.get_tools()
            tool_count = len(tools)
        except Exception as e:
            print(f"ë„êµ¬ ê°œìˆ˜ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì„¤ì • íŒŒì¼ì˜ ë„êµ¬ ê°œìˆ˜ë¥¼ ì‚¬ìš©
            tool_count = len(config)
    
    return SettingsResponse(
        tool_count=tool_count,
        available_models=list(OUTPUT_TOKEN_INFO.keys()),
        current_config=config
    )

@app.post("/api/settings")
async def update_settings(tool_config: ToolRequest):
    """ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤."""
    print("[POST] /api/settings")
    global tool_count  # ì „ì—­ ë³€ìˆ˜ ì‚¬ìš© ì„ ì–¸
    
    success = save_config_to_json(tool_config.tool_config)
    if not success:
        raise HTTPException(status_code=500, detail="ì„¤ì • ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    
    # ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ë©´ ë„êµ¬ ê°œìˆ˜ë„ ì—…ë°ì´íŠ¸
    try:
        # ìƒˆë¡œìš´ API ì‚¬ìš© ë°©ì‹ ì ìš©
        temp_client = MultiServerMCPClient(tool_config.tool_config)
        tools = await temp_client.get_tools()
        tool_count = len(tools)
    except Exception as e:
        print(f"ë„êµ¬ ê°œìˆ˜ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì„¤ì • íŒŒì¼ì˜ ë„êµ¬ ê°œìˆ˜ë¥¼ ì‚¬ìš©
        tool_count = len(tool_config.tool_config)
    
    return {
        "success": True, 
        "message": "ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.", 
        "tool_count": tool_count
    }

@app.post("/api/threads")
async def create_thread():
    """ìƒˆë¡œìš´ ëŒ€í™” ìŠ¤ë ˆë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print("[POST] /api/threads")
    thread_id = random_uuid()
    await initialize_agent(thread_id)
    return ThreadResponse(thread_id=thread_id)

@app.get("/api/threads/{thread_id}")
async def get_thread(thread_id: str):
    """íŠ¹ì • ìŠ¤ë ˆë“œì˜ ëŒ€í™” ê¸°ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    print("[GET] /api/threads/{thread_id}")
    if thread_id not in conversation_histories:
        raise HTTPException(status_code=404, detail="ìŠ¤ë ˆë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return MessageResponse(
        messages=conversation_histories[thread_id],
        thread_id=thread_id
    )

@app.delete("/api/threads/{thread_id}")
async def delete_thread(thread_id: str):
    """ìŠ¤ë ˆë“œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    print("[DELETE] /api/threads/{thread_id}")
    if thread_id in mcp_clients:
        # ìƒˆ ë²„ì „ì—ì„œëŠ” context manager ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì§ì ‘ ì‚­ì œë§Œ í•¨
        del mcp_clients[thread_id]
    
    if thread_id in agents:
        del agents[thread_id]
    
    if thread_id in conversation_histories:
        del conversation_histories[thread_id]
    
    return {"success": True, "message": "ìŠ¤ë ˆë“œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}

@app.post("/api/upload", response_model=FileUploadResponse)
async def upload_file(file: UploadFile = File(...)):
    """
    íŒŒì¼ì„ ì„œë²„ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    """
    print(f"[POST] /api/upload - íŒŒì¼ ì´ë¦„: {file.filename}, í¬ê¸°: {file.size if hasattr(file, 'size') else 'ì•Œ ìˆ˜ ì—†ìŒ'}")
    try:
        # ê³ ìœ  íŒŒì¼ëª… ìƒì„± (íŒŒì¼ëª… ì¶©ëŒ ë°©ì§€)
        file_extension = os.path.splitext(file.filename)[1] if file.filename else ""
        unique_filename = f"{uuid.uuid4()}{file_extension}"
        file_path = os.path.join(ABSOLUTE_UPLOAD_DIR, unique_filename)
        
        # íŒŒì¼ ì €ì¥
        with open(file_path, "wb") as f:
            shutil.copyfileobj(file.file, f)
        
        # íŒŒì¼ ì •ë³´ ë°˜í™˜
        file_size = os.path.getsize(file_path)
        print(f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {file_path}, í¬ê¸°: {file_size}")
        
        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        response_data = FileUploadResponse(
            file_path=file_path,  # ì ˆëŒ€ ê²½ë¡œë¡œ ë°˜í™˜
            file_name=file.filename,
            file_size=file_size
        )
        print(f"ì‘ë‹µ ë°ì´í„°: {response_data}")
        return response_data
    except Exception as e:
        print(f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    finally:
        file.file.close()

@app.post("/api/threads/{thread_id}/query")
async def query_agent(thread_id: str, request: QueryRequest, background_tasks: BackgroundTasks):
    """
    ì—ì´ì „íŠ¸ì— ì§ˆë¬¸ì„ ì „ì†¡í•˜ê³  ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"[POST] /api/threads/{thread_id}/query: {request.query}")
    # ìŠ¤ë ˆë“œ IDê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ìƒˆë¡œ ìƒì„±
    if not thread_id:
        thread_id = random_uuid()
    
    # ì—ì´ì „íŠ¸ê°€ ì—†ê±°ë‚˜ ëª¨ë¸ì´ ë³€ê²½ëœ ê²½ìš° ì¬ì´ˆê¸°í™”
    need_init = (
        thread_id not in agents or 
        thread_id not in agent_models or 
        agent_models.get(thread_id) != request.model
    )
    
    if need_init:
        print(f"ì—ì´ì „íŠ¸ ì´ˆê¸°í™”/ì¬ì´ˆê¸°í™” í•„ìš”: thread_id={thread_id}, model={request.model}")
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        success = await initialize_agent(
            thread_id, 
            model=request.model
        )
        
        if not success:
            raise HTTPException(status_code=500, detail="ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    
    # ì²¨ë¶€ íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ
    attachment_path = None
    if "[ì²¨ë¶€ íŒŒì¼: " in request.query:
        try:
            # ì²¨ë¶€ íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ
            start_idx = request.query.find("[ì²¨ë¶€ íŒŒì¼: ") + len("[ì²¨ë¶€ íŒŒì¼: ")
            end_idx = request.query.find("]", start_idx)
            attachment_path = request.query[start_idx:end_idx]
            
            # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if attachment_path.startswith("./uploads/") or attachment_path.startswith("/uploads/"):
                file_name = os.path.basename(attachment_path)
                attachment_path = os.path.join(ABSOLUTE_UPLOAD_DIR, file_name)
                print(f"ì²¨ë¶€ íŒŒì¼ ê²½ë¡œ ë³€í™˜: {attachment_path}")
            
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not os.path.exists(attachment_path):
                print(f"ê²½ê³ : íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {attachment_path}")
                return {
                    "response": f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ: {attachment_path}",
                    "thread_id": thread_id
                }
            else:
                print(f"íŒŒì¼ í™•ì¸ ì™„ë£Œ: {attachment_path} (í¬ê¸°: {os.path.getsize(attachment_path)} bytes)")
        except Exception as e:
            print(f"ì²¨ë¶€ íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    try:
        # ëŒ€ìš©ëŸ‰ íŒŒì¼ì´ í¬í•¨ëœ ê²½ìš° íƒ€ì„ì•„ì›ƒ ê°’ ì¦ê°€
        timeout_value = request.timeout_seconds
        if attachment_path and os.path.exists(attachment_path):
            file_size = os.path.getsize(attachment_path)
            # íŒŒì¼ í¬ê¸°ê°€ 10MBë¥¼ ì´ˆê³¼í•˜ë©´ íƒ€ì„ì•„ì›ƒ ê°’ ì¦ê°€
            if file_size > 10 * 1024 * 1024:
                timeout_value = max(timeout_value, 300)  # ìµœì†Œ 300ì´ˆ
                print(f"ëŒ€ìš©ëŸ‰ íŒŒì¼ ê°ì§€: íƒ€ì„ì•„ì›ƒ ê°’ì„ {timeout_value}ì´ˆë¡œ ì¦ê°€")
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘ ë¡œê¹…
        print(f"ëŒ€í™” ì²˜ë¦¬ ì‹œì‘: íƒ€ì„ì•„ì›ƒ={timeout_value}ì´ˆ, ì¬ê·€ ì œí•œ={request.recursion_limit}")
        
        # ì§ˆë¬¸ ì²˜ë¦¬
        _, final_text, final_tool = await process_query(
            thread_id, 
            request.query,
            timeout_seconds=timeout_value,
            recursion_limit=request.recursion_limit
        )
        
        print(f"final_text ê¸¸ì´: {len(final_text)}")
        if final_tool:
            print(f"final_tool ê¸¸ì´: {len(final_tool)}")
            print(f"final_tool ìƒ˜í”Œ: {final_tool[:200]}..." if len(final_tool) > 200 else final_tool)
        
        # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        conversation_histories[thread_id].append(
            Message(
                role="user", 
                content=request.query,
                attachment_path=attachment_path
            )
        )
        conversation_histories[thread_id].append(
            Message(
                role="assistant", 
                content=final_text
            )
        )
        
        # ë„êµ¬ ì •ë³´ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì •ë³´ ë°˜í™˜
        tool_info = None
        if final_tool:
            tool_info = final_tool
            print(f"ë„êµ¬ ì‚¬ìš© ì •ë³´ ì „ì†¡: ë„êµ¬ ì •ë³´ ê¸¸ì´ {len(tool_info)}")
        
        return QueryResponse(
            response=final_text,
            tool_info=tool_info,
            thread_id=thread_id
        )
    except asyncio.TimeoutError:
        error_msg = f"ìš”ì²­ ì‹œê°„ì´ {timeout_value}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        print(f"ì˜¤ë¥˜: íƒ€ì„ì•„ì›ƒ ë°œìƒ - {error_msg}")
        
        # íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ ëŒ€í™” ê¸°ë¡ì— ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ê°€
        conversation_histories[thread_id].append(
            Message(
                role="user", 
                content=request.query,
                attachment_path=attachment_path
            )
        )
        conversation_histories[thread_id].append(
            Message(
                role="assistant", 
                content=error_msg
            )
        )
        
        return QueryResponse(
            response=error_msg,
            thread_id=thread_id
        )
    except Exception as e:
        import traceback
        error_msg = f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n{traceback.format_exc()}"
        print(f"ì˜¤ë¥˜: {error_msg}")
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ëŒ€í™” ê¸°ë¡ì— ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ê°€
        conversation_histories[thread_id].append(
            Message(
                role="user", 
                content=request.query,
                attachment_path=attachment_path
            )
        )
        conversation_histories[thread_id].append(
            Message(
                role="assistant", 
                content=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            )
        )
        
        return QueryResponse(
            response=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            thread_id=thread_id
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 