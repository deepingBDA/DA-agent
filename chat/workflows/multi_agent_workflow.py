"""
Multi-Agent LangGraph Workflow
===============================

LangGraphë¥¼ ì‚¬ìš©í•œ ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° êµ¬í˜„ì…ë‹ˆë‹¤.
ì¡°ê±´ë¶€ ë¼ìš°íŒ…ê³¼ ìƒíƒœ ê´€ë¦¬ë¥¼ í†µí•´ ë³µì¡í•œ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

from typing import TypedDict, List, Dict, Any, Optional, Annotated
from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
import asyncio
from datetime import datetime
import json
import logging

# ìƒíƒœ íƒ€ì… ì •ì˜
class MultiAgentState(TypedDict):
    """ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""
    # ê¸°ë³¸ ì •ë³´
    user_query: str
    session_id: str
    timestamp: datetime
    
    # ì˜ë„ ë¶„ì„ ê²°ê³¼
    intent: Dict[str, Any]
    metadata: Dict[str, Any]
    
    # ì‘ì—… ê´€ë¦¬
    tasks: List[Dict[str, Any]]
    completed_tasks: List[str]
    current_task: Optional[Dict[str, Any]]
    
    # ì—ì´ì „íŠ¸ ê²°ê³¼
    data_analysis_result: Optional[Dict[str, Any]]
    insight_analysis_result: Optional[Dict[str, Any]]
    recommendation_result: Optional[Dict[str, Any]]
    anomaly_result: Optional[Dict[str, Any]]
    trend_result: Optional[Dict[str, Any]]
    
    # ìµœì¢… ê²°ê³¼
    final_insight: str
    confidence_score: float
    processing_log: List[str]
    
    # ì˜¤ë¥˜ ì²˜ë¦¬
    errors: List[str]
    retry_count: int
    
    # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ (LangGraph í˜¸í™˜)
    messages: Annotated[list, add_messages]

class MultiAgentWorkflow:
    """ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, mcp_client=None, model=None):
        """
        Args:
            mcp_client: MCP í´ë¼ì´ì–¸íŠ¸ (ë„êµ¬ ì ‘ê·¼ìš©)
            model: LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        """
        self.mcp_client = mcp_client
        self.model = model
        self.logger = self._setup_logging()
        
        # ì²´í¬í¬ì¸í„° ì„¤ì • (ìƒíƒœ ì €ì¥ìš©)
        self.checkpointer = MemorySaver()
        
        # ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ë¹Œë“œ
        self.graph = self._build_workflow_graph()
        self.compiled_graph = self.graph.compile(checkpointer=self.checkpointer)
        
        # ì—ì´ì „íŠ¸ ì „ë¬¸ì„± ë§¤í•‘
        self.agent_capabilities = {
            "data_analyst": ["data_collection", "statistical_analysis", "data_validation"],
            "insight_generator": ["pattern_recognition", "business_analysis", "root_cause_analysis"], 
            "recommendation": ["action_planning", "optimization", "roi_calculation"],
            "anomaly_detector": ["outlier_detection", "threshold_analysis", "alert_generation"],
            "trend_predictor": ["forecasting", "trend_analysis", "seasonality_detection"]
        }
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger("multi_agent_workflow")
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - MultiAgent - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _build_workflow_graph(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        workflow = StateGraph(MultiAgentState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("analyze_intent", self._analyze_intent_node)
        workflow.add_node("decompose_tasks", self._decompose_tasks_node)  
        workflow.add_node("data_collection", self._data_collection_node)
        workflow.add_node("insight_generation", self._insight_generation_node)
        workflow.add_node("recommendation_generation", self._recommendation_generation_node)
        workflow.add_node("anomaly_detection", self._anomaly_detection_node)
        workflow.add_node("trend_analysis", self._trend_analysis_node)
        workflow.add_node("synthesize_results", self._synthesize_results_node)
        workflow.add_node("error_handler", self._error_handler_node)
        
        # ì—£ì§€ ì—°ê²°
        workflow.add_edge(START, "analyze_intent")
        workflow.add_edge("analyze_intent", "decompose_tasks")
        
        # ì¡°ê±´ë¶€ ë¼ìš°íŒ… - ì˜ë„ì— ë”°ë¥¸ ë¶„ê¸°
        workflow.add_conditional_edges(
            "decompose_tasks",
            self._route_by_intent,
            {
                "data_first": "data_collection",
                "anomaly_first": "anomaly_detection", 
                "trend_first": "trend_analysis",
                "direct_insight": "insight_generation",
                "direct_synthesis": "synthesize_results",  # ê°„ë‹¨í•œ ì§ˆë¬¸ì€ ë°”ë¡œ í†µí•©
                "error": "error_handler"
            }
        )
        
        # ë°ì´í„° ìˆ˜ì§‘ í›„ ë¶„ê¸°
        workflow.add_conditional_edges(
            "data_collection",
            self._route_after_data_collection,
            {
                "to_insights": "insight_generation",
                "to_anomaly": "anomaly_detection",
                "to_trend": "trend_analysis", 
                "to_synthesis": "synthesize_results",
                "error": "error_handler"
            }
        )
        
        # ê° ë¶„ì„ ë‹¨ê³„ì—ì„œ í†µí•©ìœ¼ë¡œ
        workflow.add_conditional_edges(
            "insight_generation",
            self._route_after_analysis,
            {
                "continue": "recommendation_generation",
                "synthesize": "synthesize_results",
                "error": "error_handler"
            }
        )
        
        workflow.add_conditional_edges(
            "recommendation_generation", 
            self._route_after_analysis,
            {
                "synthesize": "synthesize_results",
                "error": "error_handler"
            }
        )
        
        workflow.add_conditional_edges(
            "anomaly_detection",
            self._route_after_analysis, 
            {
                "continue": "insight_generation",
                "synthesize": "synthesize_results",
                "error": "error_handler"
            }
        )
        
        workflow.add_conditional_edges(
            "trend_analysis",
            self._route_after_analysis,
            {
                "continue": "insight_generation", 
                "synthesize": "synthesize_results",
                "error": "error_handler"
            }
        )
        
        # ìµœì¢… ë‹¨ê³„ë“¤
        workflow.add_edge("synthesize_results", END)
        workflow.add_edge("error_handler", END)
        
        return workflow
    
    # ========================================================================
    # ë…¸ë“œ êµ¬í˜„ë¶€
    # ========================================================================
    
    async def _analyze_intent_node(self, state: MultiAgentState) -> MultiAgentState:
        """ì˜ë„ ë¶„ì„ ë…¸ë“œ - GPT ëª¨ë¸ ì‚¬ìš©"""
        try:
            self.logger.info("ì˜ë„ ë¶„ì„ ì‹œì‘")
            
            user_query = state["user_query"]
            
            # ê°„ë‹¨í•œ ì§ˆë¬¸ë“¤ì€ ë¹ ë¥¸ ì²˜ë¦¬
            simple_patterns = {
                "greeting": ["ì•ˆë…•", "hello", "hi", "ì¢‹ì€", "ë°˜ê°€", "í—¬ë¡œ"],
                "thanks": ["ê³ ë§ˆì›Œ", "ê°ì‚¬", "thank"],  
                "test": ["í…ŒìŠ¤íŠ¸", "test", "í™•ì¸"],
                "simple": ["ë­", "ì–´ë–»ê²Œ", "ë­”ë°", "ë¬´ì—‡"]
            }
            
            query_lower = user_query.lower()
            for category, keywords in simple_patterns.items():
                if any(keyword in query_lower for keyword in keywords):
                    if len(user_query) < 20:  # ì§§ì€ ì§ˆë¬¸
                        self.logger.info(f"ê°„ë‹¨í•œ ì§ˆë¬¸ ê°ì§€: {category}")
                        state["intent"] = {
                            "primary": "simple_response", 
                            "secondary": [category],
                            "confidence": 0.9,
                            "is_simple": True
                        }
                        state["processing_log"].append("ê°„ë‹¨í•œ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¥˜ë¨")
                        return state
            
            if self.model:
                # GPT ëª¨ë¸ë¡œ ì˜ë„ ë¶„ì„
                intent_prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì£¼ìš” ì˜ë„ë¥¼ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: "{user_query}"

ë‹¤ìŒ ì˜ë„ ì¤‘ì—ì„œ ê°€ì¥ ì í•©í•œ ê²ƒë“¤ì„ ì„ íƒí•˜ê³  ìš°ì„ ìˆœìœ„ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”:
1. diagnostic - í˜„ì¬ ìƒíƒœ ì§„ë‹¨
2. comparative - ë¹„êµ ë¶„ì„  
3. trend - íŠ¸ë Œë“œ/ì‹œê³„ì—´ ë¶„ì„
4. predictive - ì˜ˆì¸¡ ë¶„ì„
5. optimization - ê°œì„ /ìµœì í™” ì œì•ˆ
6. anomaly - ì´ìƒ íƒì§€

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{"primary": "ê°€ì¥_ì¤‘ìš”í•œ_ì˜ë„", "secondary": ["ë¶€ì°¨ì _ì˜ë„ë“¤"], "confidence": 0.9}}
"""
                
                try:
                    response = await self.model.ainvoke(intent_prompt)
                    import json
                    intent_analysis = json.loads(response.content)
                    
                    state["intent"] = intent_analysis
                    state["processing_log"].append(f"GPT ì˜ë„ ë¶„ì„ ì™„ë£Œ: {intent_analysis['primary']}")
                    self.logger.info(f"GPT ì˜ë„ ë¶„ì„ ì™„ë£Œ: {intent_analysis}")
                    
                    # ë©”íƒ€ë°ì´í„° ì²˜ë¦¬
                    metadata = {
                        "time_period": "this_week",
                        "metrics": [],
                        "urgency": "normal"
                    }
                    
                    # ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ ê²€ì¶œ
                    if any(word in user_query for word in ["ì˜¤ëŠ˜", "today"]):
                        metadata["time_period"] = "today"
                    elif any(word in user_query for word in ["ì–´ì œ", "yesterday"]):
                        metadata["time_period"] = "yesterday"
                    elif any(word in user_query for word in ["ì§€ë‚œ ì£¼", "last week"]):
                        metadata["time_period"] = "last_week"
                    
                    # ë©”íŠ¸ë¦­ í‚¤ì›Œë“œ ê²€ì¶œ
                    metric_keywords = {
                        "ë°©ë¬¸ê°": "visitors", "ë§¤ì¶œ": "sales", "ì „í™˜ìœ¨": "conversion",
                        "í”½ì—…": "pickup", "ì²´ë¥˜": "dwell_time"
                    }
                    
                    for korean, english in metric_keywords.items():
                        if korean in user_query:
                            metadata["metrics"].append(english)
                    
                    state["metadata"] = metadata
                    
                    # GPT ë¶„ì„ ì„±ê³µì‹œ ì¦‰ì‹œ ë¦¬í„´
                    return state
                    
                except Exception as e:
                    self.logger.warning(f"GPT ì˜ë„ ë¶„ì„ ì‹¤íŒ¨, í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°±: {e}")
                    # í´ë°±: í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
                    intent_patterns = {
                        "diagnostic": ["í˜„ì¬", "ìƒíƒœ", "ì–´ë–»ê²Œ", "ë¶„ì„"],
                        "comparative": ["ë¹„êµ", "ëŒ€ë¹„", "ì°¨ì´", "vs", "ì§€ë‚œ"],
                        "trend": ["íŠ¸ë Œë“œ", "ì¶”ì„¸", "ë³€í™”", "íŒ¨í„´", "ì‹œê°„"],
                        "predictive": ["ì˜ˆì¸¡", "ì „ë§", "ì•ìœ¼ë¡œ", "ë¯¸ë˜"],
                        "optimization": ["ê°œì„ ", "ìµœì í™”", "í–¥ìƒ", "ë°©ë²•"],
                        "anomaly": ["ì´ìƒ", "ë¬¸ì œ", "ê¸‰ì¦", "ê¸‰ê°", "ê°‘ìê¸°"]
                    }
                    
                    detected_intents = {}
                    for intent_type, keywords in intent_patterns.items():
                        score = sum(1 for keyword in keywords if keyword in user_query)
                        if score > 0:
                            detected_intents[intent_type] = score
                    
                    primary_intent = max(detected_intents.items(), key=lambda x: x[1])[0] if detected_intents else "diagnostic"
                    
                    state["intent"] = {
                        "primary": primary_intent,
                        "secondary": list(detected_intents.keys()),
                        "confidence": 0.7
                    }
                    
                    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    metadata = {
                        "time_period": "this_week",  # ê¸°ë³¸ê°’
                        "metrics": [],
                        "urgency": "normal"
                    }
                    
                    # ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ ê²€ì¶œ
                    if any(word in user_query for word in ["ì˜¤ëŠ˜", "today"]):
                        metadata["time_period"] = "today"
                    elif any(word in user_query for word in ["ì–´ì œ", "yesterday"]):
                        metadata["time_period"] = "yesterday"
                    elif any(word in user_query for word in ["ì§€ë‚œ ì£¼", "last week"]):
                        metadata["time_period"] = "last_week"
                    
                    # ë©”íŠ¸ë¦­ í‚¤ì›Œë“œ ê²€ì¶œ
                    metric_keywords = {
                        "ë°©ë¬¸ê°": "visitors", "ë§¤ì¶œ": "sales", "ì „í™˜ìœ¨": "conversion",
                        "í”½ì—…": "pickup", "ì²´ë¥˜": "dwell_time"
                    }
                    
                    for korean, english in metric_keywords.items():
                        if korean in user_query:
                            metadata["metrics"].append(english)
                    
                    state["metadata"] = metadata
                    state["processing_log"].append(f"ì˜ë„ ë¶„ì„ ì™„ë£Œ: {primary_intent}")
                    
            else:
                # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                state["intent"] = {
                    "primary": "diagnostic",
                    "secondary": [],
                    "confidence": 0.5
                }
                state["metadata"] = {
                    "time_period": "this_week",
                    "metrics": [],
                    "urgency": "normal"
                }
            
            return state
            
        except Exception as e:
            self.logger.error(f"ì˜ë„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            state["errors"].append(f"Intent analysis failed: {str(e)}")
            return state
    
    async def _decompose_tasks_node(self, state: MultiAgentState) -> MultiAgentState:
        """ì‘ì—… ë¶„í•´ ë…¸ë“œ"""
        try:
            self.logger.info("ì‘ì—… ë¶„í•´ ì‹œì‘")
            
            primary_intent = state["intent"]["primary"]
            metadata = state["metadata"]
            
            tasks = []
            
            # ì˜ë„ë³„ ì‘ì—… ë¶„í•´ ì „ëµ
            if primary_intent == "diagnostic":
                tasks = [
                    {"type": "data_collection", "priority": 1, "agent": "data_analyst"},
                    {"type": "insight_generation", "priority": 2, "agent": "insight_generator"},
                    {"type": "recommendation", "priority": 3, "agent": "recommendation"}
                ]
            
            elif primary_intent == "comparative":
                tasks = [
                    {"type": "comparative_data_collection", "priority": 1, "agent": "data_analyst"},
                    {"type": "comparative_analysis", "priority": 2, "agent": "insight_generator"},
                    {"type": "recommendation", "priority": 3, "agent": "recommendation"}
                ]
            
            elif primary_intent == "trend":
                tasks = [
                    {"type": "time_series_collection", "priority": 1, "agent": "data_analyst"},
                    {"type": "trend_analysis", "priority": 2, "agent": "trend_predictor"},
                    {"type": "insight_generation", "priority": 3, "agent": "insight_generator"}
                ]
            
            elif primary_intent == "anomaly":
                tasks = [
                    {"type": "anomaly_detection", "priority": 1, "agent": "anomaly_detector"},
                    {"type": "data_validation", "priority": 2, "agent": "data_analyst"},
                    {"type": "insight_generation", "priority": 3, "agent": "insight_generator"}
                ]
            
            elif primary_intent == "optimization":
                tasks = [
                    {"type": "performance_analysis", "priority": 1, "agent": "data_analyst"}, 
                    {"type": "optimization_analysis", "priority": 2, "agent": "insight_generator"},
                    {"type": "action_planning", "priority": 3, "agent": "recommendation"}
                ]
            
            else:  # predictive or default
                tasks = [
                    {"type": "historical_data_collection", "priority": 1, "agent": "data_analyst"},
                    {"type": "predictive_analysis", "priority": 2, "agent": "trend_predictor"},
                    {"type": "recommendation", "priority": 3, "agent": "recommendation"}
                ]
            
            # ì‘ì—…ì— IDì™€ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for i, task in enumerate(tasks):
                task["task_id"] = f"task_{i+1}_{datetime.now().strftime('%H%M%S')}"
                task["metadata"] = metadata.copy()
                task["status"] = "pending"
            
            state["tasks"] = tasks
            state["completed_tasks"] = []
            state["processing_log"].append(f"ì‘ì—… ë¶„í•´ ì™„ë£Œ: {len(tasks)}ê°œ ì‘ì—… ìƒì„±")
            
            return state
            
        except Exception as e:
            self.logger.error(f"ì‘ì—… ë¶„í•´ ì˜¤ë¥˜: {e}")
            state["errors"].append(f"Task decomposition failed: {str(e)}")
            return state
    
    async def _data_collection_node(self, state: MultiAgentState) -> MultiAgentState:
        """ë°ì´í„° ìˆ˜ì§‘ ë…¸ë“œ - MCP íˆ´ í™œìš©"""
        try:
            self.logger.info("ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
            
            # MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•œ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘
            if self.mcp_client:
                # ì‹¤ì œ íˆ´ í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
                await asyncio.sleep(1)  # ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„
                
                # ìƒ˜í”Œ ë°ì´í„° (ì‹¤ì œë¡œëŠ” MCP íˆ´ ê²°ê³¼)
                data_result = {
                    "daily_visitors": [1200, 1150, 1300, 1250, 1400, 1100, 980],
                    "conversion_rate": 0.34,
                    "pickup_rate": 0.12,
                    "avg_dwell_time": 8.5,
                    "top_zones": ["ìŒë£Œ", "ê³¼ì", "ì•„ì´ìŠ¤í¬ë¦¼"],
                    "data_quality": 0.89,
                    "sample_size": 7892,
                    "collection_timestamp": datetime.now().isoformat()
                }
            else:
                # ì‹œë®¬ë ˆì´ì…˜ëœ ë°ì´í„°
                data_result = {
                    "daily_visitors": [1000, 1100, 1050, 1200, 1150],
                    "conversion_rate": 0.32,
                    "pickup_rate": 0.10,
                    "avg_dwell_time": 7.2,
                    "data_quality": 0.75,
                    "sample_size": 5000
                }
            
            state["data_analysis_result"] = data_result
            state["completed_tasks"].append("data_collection")
            state["processing_log"].append("ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
            
            return state
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            state["errors"].append(f"Data collection failed: {str(e)}")
            return state
    
    async def _insight_generation_node(self, state: MultiAgentState) -> MultiAgentState:
        """ì¸ì‚¬ì´íŠ¸ ìƒì„± ë…¸ë“œ"""
        try:
            self.logger.info("ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œì‘")
            
            data_result = state.get("data_analysis_result", {})
            intent = state.get("intent", {})
            
            # ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±
            insights = []
            
            if "daily_visitors" in data_result:
                visitors = data_result["daily_visitors"]
                if len(visitors) >= 2:
                    recent_avg = sum(visitors[-3:]) / 3 if len(visitors) >= 3 else visitors[-1]
                    prev_avg = sum(visitors[:-3]) / len(visitors[:-3]) if len(visitors) > 3 else visitors[0]
                    
                    if recent_avg > prev_avg * 1.1:
                        insights.append({
                            "type": "positive_trend",
                            "message": f"ìµœê·¼ ë°©ë¬¸ê°ì´ {((recent_avg/prev_avg-1)*100):.1f}% ì¦ê°€ ì¶”ì„¸",
                            "confidence": 0.85,
                            "priority": "high"
                        })
                    elif recent_avg < prev_avg * 0.9:
                        insights.append({
                            "type": "negative_trend", 
                            "message": f"ìµœê·¼ ë°©ë¬¸ê°ì´ {((1-recent_avg/prev_avg)*100):.1f}% ê°ì†Œ ìš°ë ¤",
                            "confidence": 0.87,
                            "priority": "critical"
                        })
            
            if "conversion_rate" in data_result:
                conv_rate = data_result["conversion_rate"]
                if conv_rate > 0.35:
                    insights.append({
                        "type": "performance",
                        "message": f"ì „í™˜ìœ¨ {conv_rate:.1%}ë¡œ ìš°ìˆ˜í•œ ì„±ê³¼",
                        "confidence": 0.90,
                        "priority": "medium"
                    })
                elif conv_rate < 0.25:
                    insights.append({
                        "type": "performance_issue",
                        "message": f"ì „í™˜ìœ¨ {conv_rate:.1%}ë¡œ ê°œì„  í•„ìš”",
                        "confidence": 0.88,
                        "priority": "high"
                    })
            
            if "pickup_rate" in data_result:
                pickup_rate = data_result["pickup_rate"]
                if pickup_rate < 0.08:
                    insights.append({
                        "type": "engagement_issue",
                        "message": f"í”½ì—…ë¥  {pickup_rate:.1%}ë¡œ ìƒí’ˆ ë§¤ë ¥ë„ ì ê²€ í•„ìš”",
                        "confidence": 0.82,
                        "priority": "medium"
                    })
            
            insight_result = {
                "insights": insights,
                "analysis_type": intent.get("primary", "diagnostic"),
                "key_metrics": {
                    "performance_score": self._calculate_performance_score(data_result),
                    "trend_direction": self._determine_trend_direction(data_result),
                    "risk_level": self._assess_risk_level(insights)
                },
                "confidence": sum(i["confidence"] for i in insights) / len(insights) if insights else 0.5
            }
            
            state["insight_analysis_result"] = insight_result
            state["completed_tasks"].append("insight_generation")
            state["processing_log"].append(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ: {len(insights)}ê°œ")
            
            return state
            
        except Exception as e:
            self.logger.error(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            state["errors"].append(f"Insight generation failed: {str(e)}")
            return state
    
    async def _recommendation_generation_node(self, state: MultiAgentState) -> MultiAgentState:
        """ì¶”ì²œ ìƒì„± ë…¸ë“œ"""
        try:
            self.logger.info("ì¶”ì²œ ìƒì„± ì‹œì‘")
            
            insights = state.get("insight_analysis_result", {}).get("insights", [])
            data_result = state.get("data_analysis_result", {})
            
            recommendations = []
            
            # ì¸ì‚¬ì´íŠ¸ ê¸°ë°˜ ì¶”ì²œ ìƒì„±
            for insight in insights:
                if insight["type"] == "negative_trend":
                    recommendations.append({
                        "priority": "HIGH",
                        "action": "ë°©ë¬¸ê° ìœ ì… ì±„ë„ ë‹¤ê°í™” ë° í”„ë¡œëª¨ì…˜ ê°•í™”",
                        "expected_impact": "15-20% ë°©ë¬¸ê° ì¦ê°€",
                        "implementation_time": "1-2ì£¼",
                        "roi_estimate": "ì›” ë§¤ì¶œ 10-15% ì¦ëŒ€"
                    })
                
                elif insight["type"] == "performance_issue":
                    recommendations.append({
                        "priority": "CRITICAL",
                        "action": "ì „í™˜ ì¥ë²½ ë¶„ì„ ë° êµ¬ë§¤ ë™ì„  ìµœì í™”", 
                        "expected_impact": "ì „í™˜ìœ¨ 5-8% í¬ì¸íŠ¸ ê°œì„ ",
                        "implementation_time": "3-5ì¼",
                        "roi_estimate": "ì›” ìˆœì´ìµ 20-25% ì¦ê°€"
                    })
                
                elif insight["type"] == "engagement_issue":
                    recommendations.append({
                        "priority": "MEDIUM",
                        "action": "ìƒí’ˆ ì§„ì—´ ë° ì‹œê°ì  ì–´í•„ ê°•í™”",
                        "expected_impact": "í”½ì—…ë¥  3-5% í¬ì¸íŠ¸ ê°œì„ ",
                        "implementation_time": "1-2ì¼", 
                        "roi_estimate": "ì›” ë§¤ì¶œ 5-8% ì¦ê°€"
                    })
            
            # ë°ì´í„° í’ˆì§ˆ ê¸°ë°˜ ì¶”ì²œ
            data_quality = data_result.get("data_quality", 1.0)
            if data_quality < 0.8:
                recommendations.append({
                    "priority": "MEDIUM",
                    "action": "ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ ì ê²€ ë° í’ˆì§ˆ ê°œì„ ",
                    "expected_impact": "ë¶„ì„ ì •í™•ë„ í–¥ìƒ",
                    "implementation_time": "1ì£¼ì¼",
                    "roi_estimate": "ì¥ê¸°ì  ì˜ì‚¬ê²°ì • í’ˆì§ˆ ê°œì„ "
                })
            
            # ê¸°ë³¸ ì¶”ì²œì‚¬í•­
            if not recommendations:
                recommendations.append({
                    "priority": "LOW",
                    "action": "í˜„ì¬ ì„±ê³¼ ìœ ì§€ ë° ì§€ì†ì  ëª¨ë‹ˆí„°ë§",
                    "expected_impact": "ì•ˆì •ì  ìš´ì˜ ì§€ì†",
                    "implementation_time": "ì§€ì†",
                    "roi_estimate": "í˜„ ìˆ˜ì¤€ ìœ ì§€"
                })
            
            recommendation_result = {
                "recommendations": recommendations,
                "prioritized_actions": sorted(recommendations, key=lambda x: {"CRITICAL": 0, "HIGH": 1, "MEDIUM": 2, "LOW": 3}[x["priority"]]),
                "implementation_roadmap": self._create_implementation_roadmap(recommendations),
                "confidence": 0.82
            }
            
            state["recommendation_result"] = recommendation_result
            state["completed_tasks"].append("recommendation_generation")
            state["processing_log"].append(f"ì¶”ì²œ ìƒì„± ì™„ë£Œ: {len(recommendations)}ê°œ")
            
            return state
            
        except Exception as e:
            self.logger.error(f"ì¶”ì²œ ìƒì„± ì˜¤ë¥˜: {e}")
            state["errors"].append(f"Recommendation generation failed: {str(e)}")
            return state
    
    async def _anomaly_detection_node(self, state: MultiAgentState) -> MultiAgentState:
        """ì´ìƒ íƒì§€ ë…¸ë“œ"""
        try:
            self.logger.info("ì´ìƒ íƒì§€ ì‹œì‘")
            
            data_result = state.get("data_analysis_result", {})
            
            anomalies = []
            
            # ê°„ë‹¨í•œ ì´ìƒì¹˜ íƒì§€ ë¡œì§
            if "daily_visitors" in data_result:
                visitors = data_result["daily_visitors"]
                if len(visitors) > 2:
                    mean_visitors = sum(visitors) / len(visitors)
                    std_visitors = (sum((x - mean_visitors) ** 2 for x in visitors) / len(visitors)) ** 0.5
                    
                    for i, visitor_count in enumerate(visitors):
                        if abs(visitor_count - mean_visitors) > 2 * std_visitors:
                            anomalies.append({
                                "date_index": i,
                                "metric": "daily_visitors",
                                "value": visitor_count,
                                "expected_range": f"{mean_visitors - 2*std_visitors:.0f}-{mean_visitors + 2*std_visitors:.0f}",
                                "deviation": f"{((visitor_count/mean_visitors-1)*100):+.1f}%",
                                "severity": "high" if abs(visitor_count - mean_visitors) > 3 * std_visitors else "medium"
                            })
            
            anomaly_result = {
                "anomalies": anomalies,
                "anomaly_score": len(anomalies) / max(len(data_result.get("daily_visitors", [1])), 1),
                "detection_method": "statistical_threshold",
                "confidence": 0.88
            }
            
            state["anomaly_result"] = anomaly_result
            state["completed_tasks"].append("anomaly_detection")
            state["processing_log"].append(f"ì´ìƒ íƒì§€ ì™„ë£Œ: {len(anomalies)}ê°œ ë°œê²¬")
            
            return state
            
        except Exception as e:
            self.logger.error(f"ì´ìƒ íƒì§€ ì˜¤ë¥˜: {e}")
            state["errors"].append(f"Anomaly detection failed: {str(e)}")
            return state
    
    async def _trend_analysis_node(self, state: MultiAgentState) -> MultiAgentState:
        """íŠ¸ë Œë“œ ë¶„ì„ ë…¸ë“œ"""
        try:
            self.logger.info("íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
            
            data_result = state.get("data_analysis_result", {})
            
            trends = {}
            
            if "daily_visitors" in data_result:
                visitors = data_result["daily_visitors"]
                if len(visitors) >= 3:
                    # ë‹¨ìˆœ ì„ í˜• íŠ¸ë Œë“œ ê³„ì‚°
                    n = len(visitors)
                    x = list(range(n))
                    y = visitors
                    
                    # ê¸°ìš¸ê¸° ê³„ì‚°
                    slope = (n * sum(x[i] * y[i] for i in range(n)) - sum(x) * sum(y)) / (n * sum(x[i]**2 for i in range(n)) - sum(x)**2)
                    
                    trends["visitor_trend"] = {
                        "direction": "increasing" if slope > 0 else "decreasing",
                        "strength": abs(slope),
                        "forecast_next_period": visitors[-1] + slope,
                        "confidence": 0.75
                    }
            
            trend_result = {
                "trends": trends,
                "trend_summary": self._summarize_trends(trends),
                "predictions": {
                    "next_week_forecast": trends.get("visitor_trend", {}).get("forecast_next_period", "N/A"),
                    "confidence_interval": "Â±15%"
                },
                "confidence": 0.79
            }
            
            state["trend_result"] = trend_result
            state["completed_tasks"].append("trend_analysis")
            state["processing_log"].append("íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ")
            
            return state
            
        except Exception as e:
            self.logger.error(f"íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
            state["errors"].append(f"Trend analysis failed: {str(e)}")
            return state
    
    async def _synthesize_results_node(self, state: MultiAgentState) -> MultiAgentState:
        """ê²°ê³¼ í†µí•© ë…¸ë“œ - GPT ëª¨ë¸ ì‚¬ìš©"""
        try:
            self.logger.info("GPTë¥¼ ì‚¬ìš©í•œ ê²°ê³¼ í†µí•© ì‹œì‘")
            
            # ê° ì—ì´ì „íŠ¸ ê²°ê³¼ ìˆ˜ì§‘
            data_result = state.get("data_analysis_result", {})
            insight_result = state.get("insight_analysis_result", {})
            recommendation_result = state.get("recommendation_result", {})
            anomaly_result = state.get("anomaly_result", {})
            trend_result = state.get("trend_result", {})
            
            user_query = state["user_query"]
            intent = state.get("intent", {})
            is_simple = intent.get("is_simple", False)
            
            # ê°„ë‹¨í•œ ì§ˆë¬¸ì€ ë¹ ë¥¸ ì‘ë‹µ
            if is_simple:
                category = intent.get("secondary", ["greeting"])[0]
                
                simple_responses = {
                    "greeting": "ì•ˆë…•í•˜ì„¸ìš”! ë©€í‹° ì—ì´ì „íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì–´ë–¤ ë¶„ì„ì´ë‚˜ ì¸ì‚¬ì´íŠ¸ê°€ í•„ìš”í•˜ì‹ ê°€ìš”? ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.",
                    "thanks": "ì²œë§Œì—ìš”! ë” ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.",
                    "test": "ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤! ë©€í‹° ì—ì´ì „íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                    "simple": "êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶„ì„ì´ë‚˜ ì •ë³´ê°€ í•„ìš”í•˜ì‹ ê°€ìš”? ì˜ˆë¥¼ ë“¤ì–´ 'ì´ë²ˆ ì£¼ ë°©ë¬¸ê° ìˆ˜ëŠ”?', 'ë§¤ì¶œ íŠ¸ë Œë“œëŠ”?' ê°™ì€ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
                }
                
                final_insight = simple_responses.get(category, simple_responses["greeting"])
                
                state["final_insight"] = final_insight
                state["confidence_score"] = 0.9
                state["processing_log"].append("ê°„ë‹¨í•œ ì§ˆë¬¸ - ë¹ ë¥¸ ì‘ë‹µ ì™„ë£Œ")
                state["completed_tasks"] = ["quick_response"]
                
                return state
            
            if self.model:
                # GPT ëª¨ë¸ë¡œ ì „ë¬¸ì ì¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
                synthesis_prompt = f"""
ë‹¹ì‹ ì€ ë¦¬í…Œì¼ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ CEOê¸‰ ì„ì›ì§„ì´ ì½ì„ ì „ëµì  ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

## ì‚¬ìš©ì ì§ˆë¬¸
"{user_query}"

## ë¶„ì„ ì˜ë„
ì£¼ìš” ì˜ë„: {intent.get('primary', 'diagnostic')}
ë¶€ì°¨ ì˜ë„: {intent.get('secondary', [])}

## ë¶„ì„ëœ ë°ì´í„°
ë°ì´í„° ë¶„ì„ ê²°ê³¼: {data_result}
ì¸ì‚¬ì´íŠ¸ ë¶„ì„: {insight_result}
ì¶”ì²œì‚¬í•­ ê²°ê³¼: {recommendation_result}
ì´ìƒ íƒì§€ ê²°ê³¼: {anomaly_result}
íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼: {trend_result}

ë‹¤ìŒ êµ¬ì¡°ë¡œ ì „ë¬¸ì ì¸ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

# ğŸ“Š Executive Summary
**í˜„ì¬ ì„±ê³¼**: [í•µì‹¬ ì§€í‘œ ìš”ì•½]
**ì „í™˜ìœ¨**: [ì „í™˜ìœ¨ ì •ë³´ í¬í•¨ ì‹œ]

## ğŸ” Key Insights
*ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ë°œê²¬ì‚¬í•­ 3-5ê°œë¥¼ ë°ì´í„° ê·¼ê±°ì™€ í•¨ê»˜*

## ğŸ“ˆ Performance Analysis
*í˜„ì¬ ì§€í‘œì™€ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ, íŠ¸ë Œë“œ ë¶„ì„*

## âš¡ Immediate Actions (24-48ì‹œê°„)
*ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê³ ì„íŒ©íŠ¸ ê°œì„ ì•ˆ*

## ğŸ¯ Strategic Recommendations (1-4ì£¼)
*ì¤‘ì¥ê¸° ì „ëµì  ì œì•ˆê³¼ ì˜ˆìƒ ì„±ê³¼*

## ğŸ“Š Success Metrics
*ê°œì„ ë„ ì¸¡ì • ë°©ë²•ê³¼ ì¶”ì  ì§€í‘œ*

## âš ï¸ Risk Factors
*ì ì¬ì  ë„ì „ê³¼ì œì™€ ëŒ€ì‘ ë°©ì•ˆ*

**ë°ì´í„° ì†ŒìŠ¤**: [ì‚¬ìš©ëœ ë„êµ¬/í…Œì´ë¸”]
**ë¶„ì„ ê¸°ê°„**: [ë¶„ì„ ë²”ìœ„]
**ì‹ ë¢°ë„**: [ì „ì²´ì ì¸ ë¶„ì„ ì‹ ë¢°ë„]

í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë˜, ë¹„ì¦ˆë‹ˆìŠ¤ ì „ë¬¸ ìš©ì–´ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš”.
"""
                
                try:
                    response = await self.model.ainvoke(synthesis_prompt)
                    final_insight = response.content
                    
                    self.logger.info("GPT ê²°ê³¼ í†µí•© ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.warning(f"GPT ê²°ê³¼ í†µí•© ì‹¤íŒ¨, ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©: {e}")
                    # í´ë°±: ê¸°ì¡´ í•˜ë“œì½”ë”©ëœ ë¡œì§
                    final_insight = self._generate_fallback_synthesis(data_result, insight_result, recommendation_result, anomaly_result, trend_result)
            else:
                # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©
                final_insight = self._generate_fallback_synthesis(data_result, insight_result, recommendation_result, anomaly_result, trend_result)
            
            # ìš”ì•½ ì •ë³´ ìˆ˜ì§‘ìš© ë°°ì—´ ì´ˆê¸°í™”
            summary_parts = []
            
            if data_result:
                visitors = data_result.get("daily_visitors", [])
                if visitors:
                    summary_parts.append(f"ğŸ“Š **í˜„ì¬ ì„±ê³¼**: ì¼í‰ê·  ë°©ë¬¸ê° {visitors[-1]:,}ëª…")
                
                conv_rate = data_result.get("conversion_rate")
                if conv_rate:
                    summary_parts.append(f"ì „í™˜ìœ¨ {conv_rate:.1%}")
            
            # ì£¼ìš” ì¸ì‚¬ì´íŠ¸
            key_insights = []
            if insight_result and "insights" in insight_result:
                for insight in insight_result["insights"]:
                    key_insights.append(f"â€¢ {insight['message']}")
            
            # ì´ìƒ íƒì§€ ê²°ê³¼
            if anomaly_result and anomaly_result.get("anomalies"):
                key_insights.append(f"â€¢ âš ï¸ {len(anomaly_result['anomalies'])}ê°œ ì´ìƒ íŒ¨í„´ ê°ì§€")
            
            # íŠ¸ë Œë“œ ì •ë³´
            if trend_result and trend_result.get("trends"):
                trend_info = trend_result["trends"].get("visitor_trend", {})
                if trend_info:
                    direction = trend_info["direction"]
                    key_insights.append(f"â€¢ ğŸ“ˆ ë°©ë¬¸ê° ì¶”ì„¸: {'ìƒìŠ¹' if direction == 'increasing' else 'í•˜ë½'}ì„¸")
            
            # ì¶”ì²œì‚¬í•­
            top_recommendations = []
            if recommendation_result and "recommendations" in recommendation_result:
                for rec in recommendation_result["recommendations"][:3]:  # ìƒìœ„ 3ê°œ
                    top_recommendations.append(
                        f"ğŸ¯ **{rec['priority']}**: {rec['action']}\n"
                        f"   â”” ì˜ˆìƒ íš¨ê³¼: {rec['expected_impact']}"
                    )
            
            # ìµœì¢… ì¸ì‚¬ì´íŠ¸ ë¬¸ì„œ ìƒì„±
            final_parts = [
                "# ğŸ“Š Executive Summary",
                " ".join(summary_parts) if summary_parts else "ë°ì´í„° ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "",
                "## ğŸ” Key Insights"
            ]
            
            if key_insights:
                final_parts.extend(key_insights)
            else:
                final_parts.append("â€¢ íŠ¹ë³„í•œ ì´ìƒì‚¬í•­ì€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            if top_recommendations:
                final_parts.extend(["", "## ğŸ’¡ ì£¼ìš” ì¶”ì²œì‚¬í•­"])
                final_parts.extend(top_recommendations)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidences = []
            for result in [data_result, insight_result, recommendation_result, anomaly_result, trend_result]:
                if result and "confidence" in result:
                    confidences.append(result["confidence"])
            
            overall_confidence = sum(confidences) / len(confidences) if confidences else 0.5
            
            final_parts.extend([
                "",
                f"**ë¶„ì„ ì‹ ë¢°ë„**: {overall_confidence:.1%}",
                f"**ì²˜ë¦¬ ì™„ë£Œ**: {len(state['completed_tasks'])}ê°œ ì‘ì—…"
            ])
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidences = []
            for result in [data_result, insight_result, recommendation_result, anomaly_result, trend_result]:
                if result and "confidence" in result:
                    confidences.append(result["confidence"])
            
            overall_confidence = sum(confidences) / len(confidences) if confidences else 0.5
            
            state["final_insight"] = final_insight
            state["confidence_score"] = overall_confidence
            state["processing_log"].append("ìµœì¢… ê²°ê³¼ í†µí•© ì™„ë£Œ")
            
            return state
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ í†µí•© ì˜¤ë¥˜: {e}")
            state["errors"].append(f"Result synthesis failed: {str(e)}")
            state["final_insight"] = "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            return state
    
    def _generate_fallback_synthesis(self, data_result, insight_result, recommendation_result, anomaly_result, trend_result):
        """í´ë°±ìš© ê¸°ë³¸ ê²°ê³¼ í†µí•©"""
        final_parts = ["# ğŸ“Š Executive Summary"]
        
        if data_result:
            visitors = data_result.get("daily_visitors", [])
            if visitors:
                final_parts.append(f"ğŸ“Š **í˜„ì¬ ì„±ê³¼**: ì¼í‰ê·  ë°©ë¬¸ê° {visitors[-1]:,}ëª…")
            
            conv_rate = data_result.get("conversion_rate")
            if conv_rate:
                final_parts.append(f"ğŸ“Š **ì „í™˜ìœ¨**: {conv_rate:.1%}")
        
        final_parts.extend(["", "## ğŸ” Key Insights"])
        
        if insight_result and "insights" in insight_result:
            key_insights = insight_result["insights"][:3]  # ìƒìœ„ 3ê°œ
            final_parts.extend(key_insights)
        else:
            final_parts.append("â€¢ íŠ¹ë³„í•œ ì´ìƒì‚¬í•­ì€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if recommendation_result and "recommendations" in recommendation_result:
            top_recommendations = recommendation_result["recommendations"][:3]
            final_parts.extend(["", "## ğŸ’¡ ì£¼ìš” ì¶”ì²œì‚¬í•­"])
            final_parts.extend(top_recommendations)
        
        final_parts.extend([
            "",
            "**ë¶„ì„ ì‹ ë¢°ë„**: 50.0%",
            "**ì²˜ë¦¬ ì™„ë£Œ**: 2ê°œ ì‘ì—…"
        ])
        
        return "\n".join(final_parts)
    
    async def _error_handler_node(self, state: MultiAgentState) -> MultiAgentState:
        """ì˜¤ë¥˜ ì²˜ë¦¬ ë…¸ë“œ"""
        try:
            self.logger.warning("ì˜¤ë¥˜ ì²˜ë¦¬ ë…¸ë“œ ì‹¤í–‰")
            
            errors = state.get("errors", [])
            retry_count = state.get("retry_count", 0)
            
            # ì¬ì‹œë„ ë¡œì§
            if retry_count < 2:
                state["retry_count"] = retry_count + 1
                state["processing_log"].append(f"ì¬ì‹œë„ {retry_count + 1}/3")
                # ì—¬ê¸°ì„œ ì‹¤ì œë¡œëŠ” ì´ì „ ë‹¨ê³„ë¡œ ë˜ëŒì•„ê°€ê±°ë‚˜ ëŒ€ì•ˆ ì‹¤í–‰
            else:
                # ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ ì‹œ ê¸°ë³¸ ì‘ë‹µ ìƒì„±
                state["final_insight"] = (
                    "# âš ï¸ ë¶„ì„ ì˜¤ë¥˜\n\n"
                    "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì™„ì „íˆ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ê¸°ìˆ ì  ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n"
                    "ê¸°ë³¸ì ì¸ ìƒíƒœ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n"
                    "â€¢ ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤\n"
                    "â€¢ ë°ì´í„° ìˆ˜ì§‘ ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤\n"
                    "â€¢ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤\n\n"
                    f"**ì˜¤ë¥˜ ë‚´ìš©**: {'; '.join(errors[-2:])}"  # ìµœê·¼ 2ê°œ ì˜¤ë¥˜ë§Œ
                )
                state["confidence_score"] = 0.1
            
            return state
            
        except Exception as e:
            self.logger.error(f"ì˜¤ë¥˜ ì²˜ë¦¬ê¸° ìì²´ ì˜¤ë¥˜: {e}")
            state["final_insight"] = "ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            return state
    
    # ========================================================================
    # ë¼ìš°íŒ… í•¨ìˆ˜ë“¤
    # ========================================================================
    
    def _route_by_intent(self, state: MultiAgentState) -> str:
        """ì˜ë„ì— ë”°ë¥¸ ì´ˆê¸° ë¼ìš°íŒ…"""
        try:
            intent_info = state.get("intent", {})
            primary_intent = intent_info.get("primary", "diagnostic")
            is_simple = intent_info.get("is_simple", False)
            
            # ê°„ë‹¨í•œ ì§ˆë¬¸ì€ ë°”ë¡œ ê²°ê³¼ í†µí•©ìœ¼ë¡œ
            if is_simple or primary_intent == "simple_response":
                return "direct_synthesis"
            
            if primary_intent == "anomaly":
                return "anomaly_first"
            elif primary_intent == "trend": 
                return "trend_first"
            elif primary_intent in ["diagnostic", "comparative", "optimization"]:
                return "data_first"
            elif primary_intent == "predictive":
                return "trend_first"
            else:
                return "data_first"
                
        except Exception:
            return "error"
    
    def _route_after_data_collection(self, state: MultiAgentState) -> str:
        """ë°ì´í„° ìˆ˜ì§‘ í›„ ë¼ìš°íŒ…"""
        try:
            if state.get("errors"):
                return "error"
            
            primary_intent = state.get("intent", {}).get("primary", "diagnostic")
            
            if primary_intent == "anomaly":
                return "to_anomaly"
            elif primary_intent in ["trend", "predictive"]:
                return "to_trend" 
            else:
                return "to_insights"
                
        except Exception:
            return "error"
    
    def _route_after_analysis(self, state: MultiAgentState) -> str:
        """ë¶„ì„ í›„ ë¼ìš°íŒ…"""
        try:
            if state.get("errors"):
                return "error"
            
            completed = state.get("completed_tasks", [])
            
            # í•„ìˆ˜ ì‘ì—…ë“¤ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
            essential_tasks = ["data_collection", "insight_generation"]
            
            if any(task not in completed for task in essential_tasks):
                return "continue"
            else:
                return "synthesize"
                
        except Exception:
            return "error"
    
    # ========================================================================
    # í—¬í¼ í•¨ìˆ˜ë“¤
    # ========================================================================
    
    def _calculate_performance_score(self, data_result: Dict[str, Any]) -> float:
        """ì„±ê³¼ ì ìˆ˜ ê³„ì‚°"""
        score = 0.5  # ê¸°ë³¸ê°’
        
        conv_rate = data_result.get("conversion_rate", 0.3)
        pickup_rate = data_result.get("pickup_rate", 0.1)
        
        # ì „í™˜ìœ¨ ê¸°ë°˜ ì ìˆ˜ (30% ì´ìƒ ìš°ìˆ˜)
        if conv_rate >= 0.35:
            score += 0.3
        elif conv_rate >= 0.25:
            score += 0.1
        
        # í”½ì—…ë¥  ê¸°ë°˜ ì ìˆ˜ (10% ì´ìƒ ì–‘í˜¸)
        if pickup_rate >= 0.12:
            score += 0.2
        elif pickup_rate >= 0.08:
            score += 0.1
        
        return min(score, 1.0)
    
    def _determine_trend_direction(self, data_result: Dict[str, Any]) -> str:
        """íŠ¸ë Œë“œ ë°©í–¥ íŒë‹¨"""
        visitors = data_result.get("daily_visitors", [])
        if len(visitors) < 2:
            return "insufficient_data"
        
        recent = sum(visitors[-3:]) / 3 if len(visitors) >= 3 else visitors[-1]
        earlier = sum(visitors[:-3]) / len(visitors[:-3]) if len(visitors) > 3 else visitors[0]
        
        if recent > earlier * 1.05:
            return "increasing"
        elif recent < earlier * 0.95:
            return "decreasing"
        else:
            return "stable"
    
    def _assess_risk_level(self, insights: List[Dict[str, Any]]) -> str:
        """ìœ„í—˜ ìˆ˜ì¤€ í‰ê°€"""
        critical_count = sum(1 for i in insights if i.get("priority") == "critical")
        high_count = sum(1 for i in insights if i.get("priority") == "high")
        
        if critical_count > 0:
            return "critical"
        elif high_count > 1:
            return "high"
        elif high_count == 1:
            return "medium"
        else:
            return "low"
    
    def _create_implementation_roadmap(self, recommendations: List[Dict[str, Any]]) -> Dict[str, List[str]]:
        """êµ¬í˜„ ë¡œë“œë§µ ìƒì„±"""
        roadmap = {
            "immediate": [],  # 24ì‹œê°„ ë‚´
            "short_term": [],  # 1ì£¼ì¼ ë‚´
            "medium_term": []  # 1ê°œì›” ë‚´
        }
        
        for rec in recommendations:
            impl_time = rec.get("implementation_time", "")
            action = rec["action"]
            
            if any(word in impl_time for word in ["ì¦‰ì‹œ", "ë°”ë¡œ", "24ì‹œê°„", "1-2ì¼"]):
                roadmap["immediate"].append(action)
            elif any(word in impl_time for word in ["ì£¼", "week"]):
                roadmap["short_term"].append(action)
            else:
                roadmap["medium_term"].append(action)
        
        return roadmap
    
    def _summarize_trends(self, trends: Dict[str, Any]) -> str:
        """íŠ¸ë Œë“œ ìš”ì•½"""
        if not trends:
            return "ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ì–´ íŠ¸ë Œë“œë¥¼ íŒŒì•…í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        visitor_trend = trends.get("visitor_trend", {})
        if visitor_trend:
            direction = visitor_trend["direction"]
            strength = visitor_trend.get("strength", 0)
            
            if direction == "increasing":
                return f"ë°©ë¬¸ê° ìˆ˜ê°€ ìƒìŠ¹ ì¶”ì„¸ì…ë‹ˆë‹¤ (ê°•ë„: {strength:.2f})"
            else:
                return f"ë°©ë¬¸ê° ìˆ˜ê°€ í•˜ë½ ì¶”ì„¸ì…ë‹ˆë‹¤ (ê°•ë„: {strength:.2f})"
        
        return "íŠ¸ë Œë“œ íŒ¨í„´ì´ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    
    # ========================================================================
    # ê³µê°œ API
    # ========================================================================
    
    async def execute(self, user_query: str, session_id: str = None) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        try:
            session_id = session_id or f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state = {
                "user_query": user_query,
                "session_id": session_id,
                "timestamp": datetime.now(),
                "intent": {},
                "metadata": {},
                "tasks": [],
                "completed_tasks": [],
                "current_task": None,
                "data_analysis_result": None,
                "insight_analysis_result": None,
                "recommendation_result": None,
                "anomaly_result": None,
                "trend_result": None,
                "final_insight": "",
                "confidence_score": 0.0,
                "processing_log": [],
                "errors": [],
                "retry_count": 0,
                "messages": [HumanMessage(content=user_query)]
            }
            
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            config = {"configurable": {"thread_id": session_id}}
            
            final_state = None
            async for state_update in self.compiled_graph.astream(initial_state, config):
                final_state = state_update
                self.logger.info(f"ìƒíƒœ ì—…ë°ì´íŠ¸: {list(state_update.keys())}")
            
            if final_state:
                # ê²°ê³¼ì—ì„œ ë§ˆì§€ë§‰ ìƒíƒœ ì¶”ì¶œ
                last_node_state = list(final_state.values())[-1]
                
                return {
                    "success": True,
                    "session_id": session_id,
                    "final_insight": last_node_state.get("final_insight", "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."),
                    "confidence_score": last_node_state.get("confidence_score", 0.5),
                    "completed_tasks": last_node_state.get("completed_tasks", []),
                    "processing_log": last_node_state.get("processing_log", []),
                    "errors": last_node_state.get("errors", [])
                }
            else:
                return {
                    "success": False,
                    "error": "ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ìƒíƒœë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                }
            
        except Exception as e:
            self.logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "final_insight": "ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }
    
    def get_workflow_info(self) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ì •ë³´ ë°˜í™˜"""
        return {
            "nodes": list(self.graph.nodes.keys()),
            "agent_capabilities": self.agent_capabilities,
            "checkpointer_enabled": self.checkpointer is not None,
            "mcp_client_available": self.mcp_client is not None
        }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    async def test_workflow():
        workflow = MultiAgentWorkflow()
        
        result = await workflow.execute(
            "ì´ë²ˆ ì£¼ ë°©ë¬¸ê°ìˆ˜ê°€ ì–´ë–»ê²Œ ë˜ê³  ìˆë‚˜ìš”? ê°œì„  ë°©ì•ˆë„ ì•Œë ¤ì£¼ì„¸ìš”.",
            "test_session_001"
        )
        
        print("=== ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê²°ê³¼ ===")
        print(f"ì„±ê³µ: {result['success']}")
        if result['success']:
            print(f"ìµœì¢… ì¸ì‚¬ì´íŠ¸:\n{result['final_insight']}")
            print(f"ì‹ ë¢°ë„: {result['confidence_score']:.1%}")
            print(f"ì™„ë£Œëœ ì‘ì—…: {result['completed_tasks']}")
        else:
            print(f"ì˜¤ë¥˜: {result.get('error')}")
    
    # asyncio.run(test_workflow())