#!/usr/bin/env python3
"""
Database Schema Extractor
=========================

ClickHouse ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì „ì²´ ìŠ¤í‚¤ë§ˆë¥¼ ì¶”ì¶œí•˜ì—¬ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
GPT-5 ì—ì´ì „íŠ¸ê°€ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³  ìƒˆë¡œìš´ íˆ´ì„ ì œì•ˆí•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤.
"""

import json
import os
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any
import clickhouse_connect
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì´ë¯¸ êµ¬í˜„ëœ database_manager ì‚¬ìš©
sys.path.append(str(Path(__file__).parent / 'mcp_tools'))
from database_manager import get_site_client, get_all_sites

def extract_database_schema(database_name: str) -> Dict[str, Any]:
    """ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ"""
    print(f"ğŸ“Š {database_name} ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ ì‹œì‘...")
    
    schema = {
        "database": database_name,
        "extracted_at": datetime.now().isoformat(),
        "tables": {}
    }
    
    if database_name == "cu_base":
        # cu_baseëŠ” ì¤‘ì•™ DBì— ì§ì ‘ ì—°ê²°
        from database_manager import _create_config_client
        client = _create_config_client()
        if not client:
            print(f"âŒ {database_name} ì—°ê²° ì‹¤íŒ¨")
            return schema
    else:
        # plusinsightëŠ” ë§¤ì¥ë³„ DB - ì²« ë²ˆì§¸ ë§¤ì¥ ì‚¬ìš©
        sites = get_all_sites()
        if not sites:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë§¤ì¥ì´ ì—†ìŠµë‹ˆë‹¤")
            return schema
        
        print(f"ğŸ“ {sites[0]} ë§¤ì¥ì˜ {database_name} DBë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ")
        client = get_site_client(sites[0], database_name)
        if not client:
            print(f"âŒ {sites[0]} ë§¤ì¥ì˜ {database_name} ì—°ê²° ì‹¤íŒ¨")
            return schema
    
    try:
        # 1. í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
        print("ğŸ” í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ ì¤‘...")
        tables_query = f"""
        SELECT 
            name AS table_name,
            engine,
            total_rows,
            total_bytes,
            comment
        FROM system.tables 
        WHERE database = '{database_name}'
        AND name NOT LIKE '.%'  -- ìˆ¨ê¹€ í…Œì´ë¸” ì œì™¸
        ORDER BY table_name
        """
        
        tables_result = client.query(tables_query)
        print(f"âœ… {len(tables_result.result_rows)}ê°œ í…Œì´ë¸” ë°œê²¬")
        
        # 2. ê° í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
        print("ğŸ” ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ ì¤‘...")
        columns_query = f"""
        SELECT 
            table,
            name AS column_name,
            type AS data_type,
            position,
            default_kind,
            default_expression,
            comment,
            is_in_partition_key,
            is_in_sorting_key,
            is_in_primary_key
        FROM system.columns 
        WHERE database = '{database_name}'
        AND table NOT LIKE '.%'  -- ìˆ¨ê¹€ í…Œì´ë¸” ì œì™¸
        ORDER BY table, position
        """
        
        columns_result = client.query(columns_query)
        print(f"âœ… {len(columns_result.result_rows)}ê°œ ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘")
        
        # 3. ìŠ¤í‚¤ë§ˆ êµ¬ì¡°í™”
        # í…Œì´ë¸”ë³„ë¡œ ì»¬ëŸ¼ ì •ë³´ ê·¸ë£¹í™”
        columns_by_table = {}
        for row in columns_result.result_rows:
            table_name = row[0]
            if table_name not in columns_by_table:
                columns_by_table[table_name] = []
            
            columns_by_table[table_name].append({
                "name": row[1],
                "type": row[2],
                "position": row[3],
                "default_kind": row[4],
                "default_expression": row[5],
                "comment": row[6] or "",
                "is_partition_key": bool(row[7]),
                "is_sorting_key": bool(row[8]),
                "is_primary_key": bool(row[9])
            })
        
        # 4. ìµœì¢… ìŠ¤í‚¤ë§ˆ êµ¬ì„±
        for row in tables_result.result_rows:
            table_name = row[0]
            
            # ì»¬ëŸ¼ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
            columns_dict = {}
            if table_name in columns_by_table:
                for col in columns_by_table[table_name]:
                    columns_dict[col["name"]] = {
                        "type": col["type"],
                        "description": col["comment"],
                        "position": col["position"],
                        "nullable": "Nullable" in col["type"],
                        "is_primary_key": col["is_primary_key"],
                        "is_sorting_key": col["is_sorting_key"],
                        "default": col["default_expression"] if col["default_expression"] else None
                    }
            
            schema["tables"][table_name] = {
                "description": row[4] or f"{table_name} í…Œì´ë¸”",  # comment
                "engine": row[1],
                "total_rows": row[2],
                "total_bytes": row[3],
                "columns": columns_dict
            }
        
        print(f"âœ… {database_name} ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ ì™„ë£Œ: {len(schema['tables'])}ê°œ í…Œì´ë¸”")
        
    except Exception as e:
        print(f"âŒ ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        client.close()
    
    return schema

def save_schema_to_json(schema: Dict[str, Any], output_dir: Path):
    """ìŠ¤í‚¤ë§ˆë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    database_name = schema["database"]
    output_file = output_dir / f"{database_name}_schema.json"
    
    print(f"ğŸ’¾ {output_file}ì— ì €ì¥ ì¤‘...")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(schema, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… {output_file} ì €ì¥ ì™„ë£Œ ({len(schema['tables'])}ê°œ í…Œì´ë¸”)")
    
    # ë©”íƒ€ì •ë³´ íŒŒì¼ ì—…ë°ì´íŠ¸
    metadata_file = output_dir / "schema_metadata.json"
    
    # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ (ìˆë‹¤ë©´)
    metadata = {}
    if metadata_file.exists():
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
    
    # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ë©”íƒ€ì •ë³´ ì—…ë°ì´íŠ¸
    metadata[database_name] = {
        "extracted_at": schema["extracted_at"],
        "table_count": len(schema["tables"]),
        "file_path": f"{database_name}_schema.json"
    }
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ ì‹œì‘")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(__file__).parent / "chat" / "knowledge" / "schema"
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    # ì¶”ì¶œí•  ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡
    databases = ["plusinsight", "cu_base"]
    
    for database in databases:
        print(f"\n{'='*50}")
        print(f"ğŸ“Š {database} ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬ ì¤‘...")
        print(f"{'='*50}")
        
        try:
            # ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ
            schema = extract_database_schema(database)
            
            if schema["tables"]:
                # JSON íŒŒì¼ë¡œ ì €ì¥
                save_schema_to_json(schema, output_dir)
            else:
                print(f"âš ï¸ {database} ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ {database} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\nğŸ‰ ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {output_dir}")
    
    # ê²°ê³¼ ìš”ì•½
    metadata_file = output_dir / "schema_metadata.json"
    if metadata_file.exists():
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        print("\nğŸ“Š ì¶”ì¶œ ê²°ê³¼ ìš”ì•½:")
        for db_name, info in metadata.items():
            print(f"  - {db_name}: {info['table_count']}ê°œ í…Œì´ë¸” ({info['extracted_at']})")

if __name__ == "__main__":
    main()